{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-19T21:09:17.145479Z",
     "start_time": "2025-04-19T21:09:09.868547Z"
    }
   },
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# adjust paths and language\n",
    "nlp = spacy.blank(\"uk\")\n",
    "\n",
    "# load each DocBin\n",
    "db1 = DocBin().from_disk(\"../data/test.spacy\")\n",
    "db2 = DocBin().from_disk(\"../data/train.spacy\")\n",
    "\n",
    "# create a new one and add all docs\n",
    "combined = DocBin()\n",
    "for db in (db1, db2):\n",
    "    for doc in db.get_docs(nlp.vocab):\n",
    "        combined.add(doc)\n",
    "\n",
    "combined.to_disk(\"full_dataset.spacy\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T14:26:51.502613Z",
     "start_time": "2025-04-20T12:58:30.341517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.blank(\"uk\")\n",
    "INPUT_TXT = Path(\"refined.txt\")\n",
    "OUT_DIR   = Path(\"sent_spacy_bins\")\n",
    "BIN_SIZE  = 150_000  # adjust to taste\n",
    "\n",
    "# make output dir\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# stream‐in sentences, flush every BIN_SIZE docs\n",
    "doc_bin = DocBin()\n",
    "count, file_idx = 0, 0\n",
    "with INPUT_TXT.open(encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        text = line.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        doc_bin.add(nlp.make_doc(text))\n",
    "        count += 1\n",
    "        if count >= BIN_SIZE:\n",
    "            doc_bin.to_disk(OUT_DIR / f\"sentences_{file_idx}.spacy\")\n",
    "            file_idx += 1\n",
    "            doc_bin = DocBin()\n",
    "            count = 0\n",
    "\n",
    "# write any leftovers\n",
    "if count:\n",
    "    doc_bin.to_disk(OUT_DIR / f\"sentences_{file_idx}.spacy\")\n",
    "\n",
    "print(f\"Wrote {file_idx+1} bins to {OUT_DIR}/\")"
   ],
   "id": "4ffeb17e4a001635",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 17 bins to sent_spacy_bins/\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:07:44.344753Z",
     "start_time": "2025-04-23T14:05:53.289247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "nlp = spacy.blank(\"uk\")\n",
    "\n",
    "doc_bin1 = DocBin().from_disk(\"silver_spacy/UberNER1.spacy\")\n",
    "docs1 = list(doc_bin1.get_docs(nlp.vocab))\n",
    "\n",
    "doc_bin2 = DocBin().from_disk(\"silver_spacy/UberNER2.spacy\")\n",
    "docs2 = list(doc_bin2.get_docs(nlp.vocab))\n",
    "\n",
    "docs = docs1 + docs2\n",
    "\n",
    "with open(\"UberText-NER_Silver.iob\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if token.ent_type_:\n",
    "                tag = f\"{token.ent_iob_}-{token.ent_type_}\"\n",
    "            else:\n",
    "                tag = token.ent_iob_\n",
    "            f.write(f\"{token.text} {tag}\\n\")\n",
    "        f.write(\"\\n\")"
   ],
   "id": "15c2affe6e7a74ab",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:22:55.463068Z",
     "start_time": "2025-04-23T08:14:44.815468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_filtered_dataset(spacy_paths, output_path, lang=\"uk\", no_ent_limit=80000):\n",
    "    # Load blank model (to get Vocab) and read all docs\n",
    "    nlp = spacy.blank(lang)\n",
    "    all_docs = []\n",
    "    for path in spacy_paths:\n",
    "        docbin = DocBin().from_disk(path)\n",
    "        all_docs.extend(docbin.get_docs(nlp.vocab))\n",
    "    \n",
    "    # 1) Pick one example per unique entity text\n",
    "    seen = set()\n",
    "    unique_entity_docs = []\n",
    "    for doc in all_docs:\n",
    "        for ent in doc.ents:\n",
    "            text = ent.text\n",
    "            if text not in seen:\n",
    "                seen.add(text)\n",
    "                unique_entity_docs.append(doc)\n",
    "                break  # move to next doc after first new entity\n",
    "    \n",
    "    # 2) Collect docs with no entities, sample up to no_ent_limit\n",
    "    no_ent_docs = [doc for doc in all_docs if not doc.ents]\n",
    "    sampled_no_ent = random.sample(no_ent_docs, min(no_ent_limit, len(no_ent_docs)))\n",
    "    \n",
    "    # 3) Write to new DocBin, preserving entity annotations\n",
    "    new_bin = DocBin(attrs=[\"ENT_IOB\", \"ENT_TYPE\"])\n",
    "    for doc in unique_entity_docs + sampled_no_ent:\n",
    "        new_bin.add(doc)\n",
    "    new_bin.to_disk(output_path)\n",
    "    print(f\"Wrote {len(unique_entity_docs)} docs with unique entities \"\n",
    "          f\"+ {len(sampled_no_ent)} docs without entities to {output_path}\")\n",
    "\n",
    "\n",
    "files = [\"silver_spacy/UberNER1.spacy\", \"silver_spacy/UberNER2.spacy\"]\n",
    "build_filtered_dataset(files, \"silver_spacy/trimmed_UberNER.spacy\")"
   ],
   "id": "aac90c83aee2afbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 449742 docs with unique entities + 80000 docs without entities to silver_spacy/trimmed_UberNER.spacy\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T08:22:55.466738Z",
     "start_time": "2025-04-23T08:22:55.464868Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "171be34f2dc54f4e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T19:54:14.653085Z",
     "start_time": "2025-04-23T19:53:16.182487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1) Load your IOB as plain text\n",
    "ds = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\"train\": \"UberText-NER_Silver.iob\"},\n",
    "    split=\"train\",\n",
    "    keep_linebreaks=True  # keeps blank lines between sentences\n",
    ")\n",
    "\n",
    "# 2) Push to Hub, auto-sharded at ~100 MB\n",
    "ds.push_to_hub(\n",
    "    \"lang-uk/UberText-NER-Silver\",\n",
    "    max_shard_size=\"100MB\"\n",
    ")"
   ],
   "id": "48d6c9daedacf04a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0475947a084c42d5b5fab3ca77961750"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23cc741ce30e4b00bca4862c624b4d75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38656ed57bfc40cb8895a3db38cbbf22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffb181a723d64a7c8094fb66742c2e6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "22f6f696c93a4aa59716e56360413987"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1766dea79c774191a7ec9510e5d9d189"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d925bb53444242f4aac36a12b8b3e39a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9567c2b0fb5546dab7bb2880c1cec854"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33709e641368431f9b0236ad08bfb895"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5332 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2afd2093d8ef409183527309a281b983"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/lang-uk/UberText-NER_Silver/commit/2cbc1a5c5464bd92946b13d147cb5aa0c5e1abf0', commit_message='Upload dataset', commit_description='', oid='2cbc1a5c5464bd92946b13d147cb5aa0c5e1abf0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/lang-uk/UberText-NER_Silver', endpoint='https://huggingface.co', repo_type='dataset', repo_id='lang-uk/UberText-NER_Silver'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49bad19773dd8613"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
