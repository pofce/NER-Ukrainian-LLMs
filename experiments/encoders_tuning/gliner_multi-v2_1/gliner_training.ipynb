{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KeBDnZK9KHcz"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install accelerate -U\n",
    "!pip install transformers huggingface_hub\n",
    "!pip install gliner[gpu]\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from gliner import GLiNER\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f1eae566814a30bf3cf69454e41ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8928/8928 [00:00<00:00, 1781481.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2233/2233 [00:00<00:00, 1747691.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_944/3489180073.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2790' max='2790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2790/2790 12:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>27.795115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41.035200</td>\n",
       "      <td>21.674650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>41.035200</td>\n",
       "      <td>20.837543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>16.964100</td>\n",
       "      <td>22.425411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>16.964100</td>\n",
       "      <td>25.021345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>11.388100</td>\n",
       "      <td>24.418964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>11.388100</td>\n",
       "      <td>25.798410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.496600</td>\n",
       "      <td>27.727438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.306100</td>\n",
       "      <td>31.481363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.306100</td>\n",
       "      <td>29.391993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 94.12 MiB is free. Process 3189696 has 44.32 GiB memory in use. Of the allocated memory 41.77 GiB is allocated by PyTorch, and 2.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 270.12 MiB is free. Process 3189696 has 44.15 GiB memory in use. Of the allocated memory 41.92 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 212.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 108.12 MiB is free. Process 3189696 has 44.30 GiB memory in use. Of the allocated memory 41.84 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 400.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 258.12 MiB is free. Process 3189696 has 44.16 GiB memory in use. Of the allocated memory 42.22 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 346.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 250.12 MiB is free. Process 3189696 has 44.17 GiB memory in use. Of the allocated memory 42.27 GiB is allocated by PyTorch, and 1.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 426.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 212.12 MiB is free. Process 3189696 has 44.20 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 212.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 176.12 MiB is free. Process 3189696 has 44.24 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 396.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 356.12 MiB is free. Process 3189696 has 44.06 GiB memory in use. Of the allocated memory 42.05 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 312.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 150.12 MiB is free. Process 3189696 has 44.26 GiB memory in use. Of the allocated memory 41.87 GiB is allocated by PyTorch, and 1.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 316.12 MiB is free. Process 3189696 has 44.10 GiB memory in use. Of the allocated memory 41.42 GiB is allocated by PyTorch, and 2.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 212.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 130.12 MiB is free. Process 3189696 has 44.28 GiB memory in use. Of the allocated memory 42.04 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 346.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 70.12 MiB is free. Process 3189696 has 44.34 GiB memory in use. Of the allocated memory 42.26 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 232.12 MiB is free. Process 3189696 has 44.18 GiB memory in use. Of the allocated memory 40.71 GiB is allocated by PyTorch, and 2.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 212.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 76.12 MiB is free. Process 3189696 has 44.34 GiB memory in use. Of the allocated memory 42.32 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 102.12 MiB is free. Process 3189696 has 44.31 GiB memory in use. Of the allocated memory 41.24 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 138.12 MiB is free. Process 3189696 has 44.28 GiB memory in use. Of the allocated memory 42.40 GiB is allocated by PyTorch, and 1.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 126.12 MiB is free. Process 3189696 has 44.29 GiB memory in use. Of the allocated memory 42.20 GiB is allocated by PyTorch, and 1.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 214.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 142.12 MiB is free. Process 3189696 has 44.27 GiB memory in use. Of the allocated memory 41.86 GiB is allocated by PyTorch, and 1.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 212.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 96.12 MiB is free. Process 3189696 has 44.32 GiB memory in use. Of the allocated memory 42.32 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 106.12 MiB is free. Process 3189696 has 44.31 GiB memory in use. Of the allocated memory 42.33 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 312.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 56.12 MiB is free. Process 3189696 has 44.36 GiB memory in use. Of the allocated memory 41.87 GiB is allocated by PyTorch, and 1.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 212.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 38.12 MiB is free. Process 3189696 has 44.37 GiB memory in use. Of the allocated memory 42.32 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 290.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 220.12 MiB is free. Process 3189696 has 44.20 GiB memory in use. Of the allocated memory 42.20 GiB is allocated by PyTorch, and 1.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 236.12 MiB is free. Process 3189696 has 44.18 GiB memory in use. Of the allocated memory 42.79 GiB is allocated by PyTorch, and 908.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 110.12 MiB is free. Process 3189696 has 44.30 GiB memory in use. Of the allocated memory 41.31 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 312.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 104.12 MiB is free. Process 3189696 has 44.31 GiB memory in use. Of the allocated memory 41.88 GiB is allocated by PyTorch, and 1.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 426.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 62.12 MiB is free. Process 3189696 has 44.35 GiB memory in use. Of the allocated memory 42.28 GiB is allocated by PyTorch, and 1.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 132.12 MiB is free. Process 3189696 has 44.28 GiB memory in use. Of the allocated memory 42.01 GiB is allocated by PyTorch, and 1.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 346.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 12.12 MiB is free. Process 3189696 has 44.40 GiB memory in use. Of the allocated memory 42.27 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Skipping iteration due to error: CUDA out of memory. Tried to allocate 426.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 264.12 MiB is free. Process 3189696 has 44.15 GiB memory in use. Of the allocated memory 41.86 GiB is allocated by PyTorch, and 1.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Training completed successfully.\n",
      "Model is trained and returned.\n"
     ]
    }
   ],
   "source": [
    "# Assuming GLiNER and GLiNERDataset are already defined/imported\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "def create_models_directory():\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "def train_model(model_name, custom_model_name, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
    "    create_models_directory()\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = GLiNER.from_pretrained(model_name)\n",
    "\n",
    "    print(\"Loading and preparing data...\")\n",
    "\n",
    "    with open(\"train_gliner.json\", \"r\", encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "\n",
    "    with open(\"dev_gliner.json\", \"r\", encoding='utf-8') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "\n",
    "    train_dataset = GLiNERDataset(train_data, model.config, data_processor=model.data_processor)\n",
    "    test_dataset = GLiNERDataset(test_data, model.config, data_processor=model.data_processor)\n",
    "    data_collator = DataCollatorWithPadding(model.config)\n",
    "\n",
    "    if compile_model:\n",
    "        print(\"Compiling model for faster training...\")\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        model.to(device)\n",
    "        model.compile_for_training()\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        others_lr=learning_rate,\n",
    "        others_weight_decay=weight_decay,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        dataloader_num_workers=1,\n",
    "        use_cpu=(device == torch.device('cpu')),\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,           # Enable loading best model at end\n",
    "        metric_for_best_model=\"eval_loss\",      # Specify the metric to monitor\n",
    "        greater_is_better=False,                # Set based on the metric (False for loss)\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=model.data_processor.transformer_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    model.save_pretrained(f\"models/{custom_model_name}\")\n",
    "\n",
    "    print(\"Training completed successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Replace these with actual values as needed\n",
    "model_name = \"urchade/gliner_multi-v2.1\"\n",
    "custom_model_name = \"my_custom_model\"\n",
    "weight_decay = 0.05\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "epochs = 10\n",
    "compile_model = False\n",
    "\n",
    "trained_model = train_model(model_name, custom_model_name,learning_rate, weight_decay, batch_size, epochs, compile_model)\n",
    "print(\"Model is trained and returned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1WaKJOPVYDH"
   },
   "source": [
    "**Choose a model and set training parameters for your needs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1392806013aa4e9e8dca211776439b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11161/11161 [00:00<00:00, 1004972.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5087/5087 [00:00<00:00, 1771833.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_944/2630768374.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1047' max='1047' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1047/1047 04:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>35.941700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>18.191800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully.\n",
      "Model is trained and returned.\n"
     ]
    }
   ],
   "source": [
    "# Assuming GLiNER and GLiNERDataset are already defined/imported\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "def train_model(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = GLiNER.from_pretrained(model_name)\n",
    "\n",
    "    print(\"Loading and preparing data...\")\n",
    "\n",
    "    with open('full_train_gliner.json', 'r', encoding='utf-8') as file:\n",
    "        train_data = json.load(file)\n",
    "        random.seed(42)\n",
    "        random.shuffle(train_data)\n",
    "    \n",
    "    with open('test_gliner.json', 'r', encoding='utf-8') as file:\n",
    "        test_data = json.load(file)\n",
    "\n",
    "    train_dataset = GLiNERDataset(train_data, model.config, data_processor=model.data_processor)\n",
    "    test_dataset = GLiNERDataset(test_data, model.config, data_processor=model.data_processor)\n",
    "    data_collator = DataCollatorWithPadding(model.config)\n",
    "\n",
    "    if compile_model:\n",
    "        print(\"Compiling model for faster training...\")\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        model.to(device)\n",
    "        model.compile_for_training()\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        others_lr=learning_rate,\n",
    "        others_weight_decay=weight_decay,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        dataloader_num_workers=1,\n",
    "        use_cpu=(device == torch.device('cpu')),\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=model.data_processor.transformer_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    model.save_pretrained(f\"models/{custom_model_name}\")\n",
    "\n",
    "    print(\"Training completed successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Replace these with actual values as needed\n",
    "model_name = \"urchade/gliner_multi-v2.1\"\n",
    "custom_model_name = \"my_custom_model\"\n",
    "train_path = os.path.join(\"data\", \"annotated_data.json\")\n",
    "split_ratio = 0.9\n",
    "learning_rate = 0.00001\n",
    "weight_decay = 0.05\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "compile_model = False\n",
    "\n",
    "trained_model = train_model(model_name, custom_model_name, train_path, split_ratio,\n",
    "                            learning_rate, weight_decay, batch_size, epochs, compile_model)\n",
    "print(\"Model is trained and returned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8521573604060915)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/annotated_data.json', 'r') as file:\n",
    "    annotated_data = json.load(file)\n",
    "\n",
    "with open('data/test_gliner.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "all_labels = []\n",
    "for example in annotated_data:\n",
    "    ner_data = example.get(\"ner\", [])\n",
    "    for entity in ner_data:\n",
    "        label = entity[2] \n",
    "        if label not in all_labels:\n",
    "            all_labels.append(label)\n",
    "\n",
    "results, f1 = trained_model.evaluate(test_data, flat_ner=True, threshold=0.95, batch_size=12, entity_types=all_labels)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
