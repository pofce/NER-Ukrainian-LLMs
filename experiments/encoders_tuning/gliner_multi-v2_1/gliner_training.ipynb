{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KeBDnZK9KHcz"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install accelerate -U\n",
    "!pip install transformers huggingface_hub\n",
    "!pip install gliner[gpu]\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from gliner import GLiNER\n",
    "from gliner import GLiNERConfig, GLiNER\n",
    "from gliner.training import Trainer, TrainingArguments\n",
    "from gliner.data_processing.collator import DataCollatorWithPadding\n",
    "from gliner.utils import load_config_as_namespace\n",
    "from gliner.data_processing import WordsSplitter, GLiNERDataset\n",
    "\n",
    "if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "if not os.path.exists(\"data\"):\n",
    "        os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_gliner.json', 'r') as file:\n",
    "    annotated_data = json.load(file)\n",
    "\n",
    "with open('dev_gliner.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "all_labels = []\n",
    "for example in annotated_data:\n",
    "    ner_data = example.get(\"ner\", [])\n",
    "    for entity in ner_data:\n",
    "        label = entity[2] \n",
    "        if label not in all_labels:\n",
    "            all_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48427b932797417c8f0829136f4fdcd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8928/8928 [00:00<00:00, 1816921.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2233/2233 [00:00<00:00, 1874300.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1068/3877866807.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6975' max='6975' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6975/6975 34:16, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>33.599777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50.975700</td>\n",
       "      <td>24.002638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>50.975700</td>\n",
       "      <td>19.792355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>20.712600</td>\n",
       "      <td>23.617664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>20.712600</td>\n",
       "      <td>24.734045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12.717300</td>\n",
       "      <td>23.250170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>12.717300</td>\n",
       "      <td>29.923637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9.460300</td>\n",
       "      <td>28.631353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>7.477300</td>\n",
       "      <td>31.627865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.477300</td>\n",
       "      <td>33.498848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>6.047400</td>\n",
       "      <td>41.188511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>6.047400</td>\n",
       "      <td>40.933254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>5.631300</td>\n",
       "      <td>41.612091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.631300</td>\n",
       "      <td>47.507198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.081300</td>\n",
       "      <td>54.235069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>5.081300</td>\n",
       "      <td>60.241520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.362400</td>\n",
       "      <td>62.390339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.206700</td>\n",
       "      <td>61.845745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>4.206700</td>\n",
       "      <td>69.793861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.678900</td>\n",
       "      <td>71.606964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.678900</td>\n",
       "      <td>78.183456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.322100</td>\n",
       "      <td>75.921494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.322100</td>\n",
       "      <td>75.618706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.269400</td>\n",
       "      <td>74.668182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.269400</td>\n",
       "      <td>76.342613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0 ended.\n",
      "0.7037842540234885\n",
      "Epoch 2.0 ended.\n",
      "0.7948822095857027\n",
      "Epoch 3.0 ended.\n",
      "0.8270676691729324\n",
      "Epoch 4.0 ended.\n",
      "0.8530643319375713\n",
      "Epoch 5.0 ended.\n",
      "0.8666423090975521\n",
      "Epoch 6.0 ended.\n",
      "0.8702428416092788\n",
      "Epoch 7.0 ended.\n",
      "0.874061135371179\n",
      "Epoch 8.0 ended.\n",
      "0.8816412414518675\n",
      "Epoch 9.0 ended.\n",
      "0.8809688581314878\n",
      "Epoch 10.0 ended.\n",
      "0.8846834099752388\n",
      "Epoch 11.0 ended.\n",
      "0.876318975955717\n",
      "Epoch 12.0 ended.\n",
      "0.8789610389610389\n",
      "Epoch 13.0 ended.\n",
      "0.8853314772925004\n",
      "Epoch 14.0 ended.\n",
      "0.8812619171433524\n",
      "Epoch 15.0 ended.\n",
      "0.8757679180887371\n",
      "Epoch 16.0 ended.\n",
      "0.8799171842650104\n",
      "Epoch 17.0 ended.\n",
      "0.8794253463314521\n",
      "Epoch 18.0 ended.\n",
      "0.8774676650782846\n",
      "Epoch 19.0 ended.\n",
      "0.8809564474807857\n",
      "Epoch 20.0 ended.\n",
      "0.8836814399728306\n",
      "Epoch 21.0 ended.\n",
      "0.8798224650051211\n",
      "Epoch 22.0 ended.\n",
      "0.8840629274965801\n",
      "Epoch 23.0 ended.\n",
      "0.8808873720136519\n",
      "Epoch 24.0 ended.\n",
      "0.8788703640694112\n",
      "Epoch 25.0 ended.\n",
      "0.8800681431005111\n",
      "Training completed successfully.\n",
      "Model is trained and returned.\n"
     ]
    }
   ],
   "source": [
    "from gliner.data_processing.collator import DataCollatorWithPadding, DataCollator\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class MyCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Epoch {state.epoch} ended.\")\n",
    "        my_custom_function()  # Call your function here.\n",
    "        return control\n",
    "\n",
    "def my_custom_function():\n",
    "    # Your custom logic here\n",
    "    results, f1 = model.evaluate(test_data, flat_ner=True, threshold=0.95, batch_size=12, entity_types=all_labels)\n",
    "    print(f1)\n",
    "\n",
    "\n",
    "# Assuming GLiNER and GLiNERDataset are already defined/imported\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "def create_models_directory():\n",
    "    if not os.path.exists(\"models\"):\n",
    "        os.makedirs(\"models\")\n",
    "\n",
    "def train_model(model_name, custom_model_name, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
    "    \n",
    "\n",
    "    train_dataset = GLiNERDataset(train_data, model.config, data_processor=model.data_processor)\n",
    "    test_dataset = GLiNERDataset(test_data, model.config, data_processor=model.data_processor)\n",
    "\n",
    "    # use it for better performance, it mimics original implementation but it's less memory efficient\n",
    "    data_collator = DataCollatorWithPadding(model.config)\n",
    "\n",
    "    if compile_model:\n",
    "        print(\"Compiling model for faster training...\")\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        model.to(device)\n",
    "        model.compile_for_training()\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        others_lr=learning_rate,\n",
    "        others_weight_decay=weight_decay,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        dataloader_num_workers=1,\n",
    "        use_cpu=(device == torch.device('cpu')),\n",
    "        report_to=\"none\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"eval_loss\",      # Specify the metric to monitor\n",
    "        greater_is_better=False,                # Set based on the metric (False for loss)\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=model.data_processor.transformer_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[MyCallback]\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    model.save_pretrained(f\"models/{custom_model_name}\")\n",
    "\n",
    "    print(\"Training completed successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Replace these with actual values as needed\n",
    "model_name = \"urchade/gliner_multi-v2.1\"\n",
    "custom_model_name = \"my_custom_model\"\n",
    "weight_decay = 0.05\n",
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "epochs = 25\n",
    "compile_model = False\n",
    "\n",
    "create_models_directory()\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Loading model...\")\n",
    "\n",
    "model = GLiNER.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading and preparing data...\")\n",
    "with open(\"train_gliner.json\", \"r\", encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"dev_gliner.json\", \"r\", encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "trained_model = train_model(model_name, custom_model_name,learning_rate, weight_decay, batch_size, epochs, compile_model)\n",
    "print(\"Model is trained and returned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1WaKJOPVYDH"
   },
   "source": [
    "**Choose a model and set training parameters for your needs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee6dd614e14476cbf5e5396252cdf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97f090234534b42bdea4b5f4c4719a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d94d07bd33c43cc88b4eb394054609f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f372498996548848075a74aa4ada3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b9bb8547d74abcb29f6fbb0c3a742d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d6d38e50cf4b1d8b277bc09b4278fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fe062d141d4d389d81a391ea20ea98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1699c0b62c4142a618186368f2027a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11161/11161 [00:00<00:00, 1824271.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n",
      "Collecting all entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5087/5087 [00:00<00:00, 2897396.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity classes:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1203/2798357027.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4537' max='4537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4537/4537 15:29, Epoch 13/13]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>46.553200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>20.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>13.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>10.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>8.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.991600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.043500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully.\n",
      "Model is trained and returned.\n"
     ]
    }
   ],
   "source": [
    "# Assuming GLiNER and GLiNERDataset are already defined/imported\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "\n",
    "def train_model(model_name, custom_model_name, train_path, split_ratio, learning_rate, weight_decay, batch_size, epochs, compile_model):\n",
    "\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    model = GLiNER.from_pretrained(model_name)\n",
    "\n",
    "    print(\"Loading and preparing data...\")\n",
    "\n",
    "    with open('full_train_gliner.json', 'r', encoding='utf-8') as file:\n",
    "        train_data = json.load(file)\n",
    "        random.seed(42)\n",
    "        random.shuffle(train_data)\n",
    "    \n",
    "    with open('test_gliner.json', 'r', encoding='utf-8') as file:\n",
    "        test_data = json.load(file)\n",
    "\n",
    "    train_dataset = GLiNERDataset(train_data, model.config, data_processor=model.data_processor)\n",
    "    test_dataset = GLiNERDataset(test_data, model.config, data_processor=model.data_processor)\n",
    "    data_collator = DataCollatorWithPadding(model.config)\n",
    "\n",
    "    if compile_model:\n",
    "        print(\"Compiling model for faster training...\")\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        model.to(device)\n",
    "        model.compile_for_training()\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"models\",\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        others_lr=learning_rate,\n",
    "        others_weight_decay=weight_decay,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        dataloader_num_workers=1,\n",
    "        use_cpu=(device == torch.device('cpu')),\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=model.data_processor.transformer_tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    model.save_pretrained(f\"models/{custom_model_name}\")\n",
    "\n",
    "    print(\"Training completed successfully.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Replace these with actual values as needed\n",
    "model_name = \"urchade/gliner_multi-v2.1\"\n",
    "custom_model_name = \"my_custom_model\"\n",
    "train_path = os.path.join(\"data\", \"annotated_data.json\")\n",
    "split_ratio = 0.9\n",
    "learning_rate = 0.00001\n",
    "weight_decay = 0.05\n",
    "batch_size = 32\n",
    "epochs = 13\n",
    "compile_model = False\n",
    "\n",
    "trained_model = train_model(model_name, custom_model_name, train_path, split_ratio,\n",
    "                            learning_rate, weight_decay, batch_size, epochs, compile_model)\n",
    "print(\"Model is trained and returned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5087/5087 [00:43<00:00, 117.40it/s]\n"
     ]
    }
   ],
   "source": [
    "label_mapping = {\n",
    "    \"ORG\": \"organization\",\n",
    "    \"PERS\": \"person\",\n",
    "    \"LOC\": \"location\",\n",
    "    \"MON\": \"monetary value\",\n",
    "    \"PCT\": \"percentage\",\n",
    "    \"DATE\": \"date\",\n",
    "    \"TIME\": \"timestamp\",\n",
    "    \"PERIOD\": \"time period\",\n",
    "    \"JOB\": \"job title\",\n",
    "    \"DOC\": \"document name\",\n",
    "    \"QUANT\": \"quantity\",\n",
    "    \"ART\": \"artifact\",\n",
    "    \"MISC\": \"miscellaneous\"\n",
    "}\n",
    "\n",
    "labels = list(label_mapping.values())\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "results = []\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    _, text, _ = row\n",
    "    entities = trained_model.predict_entities(text, labels)\n",
    "    \n",
    "    formatted_entities = [{\n",
    "        \"label\": reverse_label_mapping[ent[\"label\"]], \n",
    "        \"text\": ent[\"text\"],\n",
    "        \"start\": ent[\"start\"],\n",
    "        \"end\": ent[\"end\"]\n",
    "    } for ent in entities]\n",
    "    \n",
    "    results.append(json.dumps(formatted_entities, ensure_ascii=False))\n",
    "\n",
    "df[\"pred\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ents_p': 0.8274439708510931,\n",
       " 'ents_r': 0.8835706944648363,\n",
       " 'ents_f': 0.8545867651235445,\n",
       " 'ents_per_type': {'LOC': {'p': 0.8774271844660194,\n",
       "   'r': 0.8838630806845966,\n",
       "   'f': 0.8806333739342267},\n",
       "  'PCT': {'p': 0.9560439560439561,\n",
       "   'r': 0.9666666666666667,\n",
       "   'f': 0.9613259668508287},\n",
       "  'JOB': {'p': 0.6111744583808438,\n",
       "   'r': 0.8158295281582952,\n",
       "   'f': 0.6988265971316819},\n",
       "  'PERS': {'p': 0.9410706252811516,\n",
       "   'r': 0.960514233241506,\n",
       "   'f': 0.9506930243126562},\n",
       "  'MISC': {'p': 0.22807017543859648,\n",
       "   'r': 0.2746478873239437,\n",
       "   'f': 0.24920127795527158},\n",
       "  'PERIOD': {'p': 0.6447368421052632,\n",
       "   'r': 0.7945945945945946,\n",
       "   'f': 0.711864406779661},\n",
       "  'ORG': {'p': 0.8369887374036752,\n",
       "   'r': 0.917478882391163,\n",
       "   'f': 0.8753874767513949},\n",
       "  'DOC': {'p': 0.7647058823529411, 'r': 0.325, 'f': 0.456140350877193},\n",
       "  'QUANT': {'p': 0.7596153846153846,\n",
       "   'r': 0.8876404494382022,\n",
       "   'f': 0.8186528497409327},\n",
       "  'DATE': {'p': 0.8471454880294659,\n",
       "   'r': 0.9181636726546906,\n",
       "   'f': 0.8812260536398466},\n",
       "  'ART': {'p': 0.7430167597765364,\n",
       "   'r': 0.5611814345991561,\n",
       "   'f': 0.639423076923077},\n",
       "  'TIME': {'p': 0.5714285714285714, 'r': 0.4, 'f': 0.47058823529411764},\n",
       "  'MON': {'p': 0.9099378881987578,\n",
       "   'r': 0.9015384615384615,\n",
       "   'f': 0.9057187017001546}}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate_ner(df, is_llm=True):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    pred_docs = []\n",
    "    for _, row in df.iterrows():\n",
    "        _, text, _, predictions = row\n",
    "\n",
    "        predictions = [(pred[\"start\"], pred[\"end\"], pred[\"label\"]) for pred in json.loads(predictions)]\n",
    "        doc = nlp(text)\n",
    "        doc.ents = [span for start, end, label in predictions\n",
    "                    if (span := doc.char_span(start, end, label=label)) is not None]\n",
    "        pred_docs.append(doc)\n",
    "\n",
    "    gold_docs = list(DocBin().from_disk(\"test.spacy\").get_docs(nlp.vocab))\n",
    "    examples = [Example(pred, gold) for pred, gold in zip(pred_docs, gold_docs)]\n",
    "    scores = Scorer().score(examples)\n",
    "    return {k: v for k, v in scores.items() if k in {\"ents_p\", \"ents_r\", \"ents_f\", \"ents_per_type\"}}\n",
    "\n",
    "evaluate_ner(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
