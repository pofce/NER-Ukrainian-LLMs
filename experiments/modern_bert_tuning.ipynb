{
 "cells": [
  {
   "cell_type": "code",
   "id": "0f0bed3a-87bb-4a26-a970-3236a4b90177",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\n",
    "!pip install flash-attn\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q datasets\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!sudo apt-get -y update\n",
    "!sudo apt-get -y install build-essential\n",
    "!pip install evaluate seqeval\n",
    "!pip install wandb"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98d07004-e33b-4670-8913-29b2c09c4136",
   "metadata": {},
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key='00f7a841cc2925bdab7c82a2b4c186d12d042cb1')\n",
    "wandb.init(project=\"Encoders\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a663386b-4730-41c4-b322-8d00c9590787",
   "metadata": {},
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Train a Named Entity Recognition (NER) model using the \"answerdotai/ModernBERT-base\" model.\n",
    "This script:\n",
    " - Loads and preprocesses IOB-formatted training data from `train.iob`\n",
    " - Splits the data into training and evaluation sets\n",
    " - Tokenizes and aligns labels to tokens\n",
    " - Defines a token-classification model using Transformers\n",
    " - Trains the model with specified hyperparameters while computing evaluation metrics\n",
    " - Saves the trained model and tokenizer\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_iob(file_path: str) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Reads an IOB file and returns lists of tokenized sentences and their corresponding labels.\n",
    "    Assumes each non-empty line contains a token and its label separated by whitespace,\n",
    "    and that sentences are separated by blank lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_tokens = []\n",
    "    current_labels = []\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if current_tokens:\n",
    "                        sentences.append(current_tokens)\n",
    "                        labels.append(current_labels)\n",
    "                        current_tokens, current_labels = [], []\n",
    "                else:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) < 2:\n",
    "                        continue  # skip malformed lines\n",
    "                    token, tag = parts[0], parts[-1]\n",
    "                    current_tokens.append(token)\n",
    "                    current_labels.append(tag)\n",
    "            # Append the last sentence if file doesn't end with a blank line\n",
    "            if current_tokens:\n",
    "                sentences.append(current_tokens)\n",
    "                labels.append(current_labels)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading file {file_path}: {e}\")\n",
    "        sys.exit(1)\n",
    "    return sentences, labels\n",
    "\n",
    "def get_label_mapping(labels: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Creates a mapping from label strings to unique integer IDs.\n",
    "    \"\"\"\n",
    "    unique_labels = set(label for sent in labels for label in sent)\n",
    "    label_to_id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
    "    return label_to_id\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for NER tasks.\n",
    "    Each example is a dict with tokenized inputs and aligned labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, sentences: List[List[str]], labels: List[List[str]], tokenizer, label_to_id, max_length: int = 128):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_to_id = label_to_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.sentences[idx]\n",
    "        label_tags = self.labels[idx]\n",
    "        # Tokenize while preserving word boundaries\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        # Align labels with tokenized inputs\n",
    "        word_ids = encoding.word_ids()\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(self.label_to_id[label_tags[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        # Remove offset mapping as it's not needed for training\n",
    "        encoding.pop(\"offset_mapping\")\n",
    "        encoding[\"labels\"] = aligned_labels\n",
    "        # Convert lists to tensors\n",
    "        for key in encoding:\n",
    "            encoding[key] = torch.tensor(encoding[key])\n",
    "        return encoding\n",
    "\n",
    "# File and hyperparameter settings\n",
    "train_file = \"train.iob\"\n",
    "output_dir = \"./ner_model\"\n",
    "max_length = 256\n",
    "num_train_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Load IOB training data\n",
    "logger.info(\"Loading training data...\")\n",
    "sentences, labels = read_iob(train_file)\n",
    "test_sentences, test_labels = read_iob(\"test.iob\")\n",
    "\n",
    "\n",
    "if not sentences:\n",
    "    logger.error(\"No data found in the training file.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Split data into training and evaluation sets (e.g., 90% train, 10% eval)\n",
    "train_sentences, train_labels = sentences, labels\n",
    "\n",
    "# Create label mappings\n",
    "label_to_id = get_label_mapping(labels)\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "# Load tokenizer and model from Hugging Face\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "logger.info(f\"Loading tokenizer and model: {model_name}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label_to_id),\n",
    "        id2label=id_to_label,\n",
    "        label2id=label_to_id\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model/tokenizer: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create training and evaluation datasets\n",
    "train_dataset = NERDataset(train_sentences, train_labels, tokenizer, label_to_id, max_length=max_length)\n",
    "test_dataset = NERDataset(test_sentences, test_labels, tokenizer, label_to_id, max_length=max_length)\n",
    "\n",
    "# Define data collator for dynamic padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds\n",
    "    pred_ids = pred_logits.argmax(axis=-1)\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for pred, label in zip(pred_ids, labels):\n",
    "        pred_seq, true_seq = [], []\n",
    "        for p, l in zip(pred, label):\n",
    "            if l != -100:  # Ignore subword/padding positions\n",
    "                pred_seq.append(id_to_label[p])\n",
    "                true_seq.append(id_to_label[l])\n",
    "        predictions.append(pred_seq)\n",
    "        true_labels.append(true_seq)\n",
    "\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    # Explicitly request 'strict' mode (exact boundary matches only) \n",
    "    # and IOB2 tagging scheme\n",
    "    results = metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=true_labels,\n",
    "        mode=\"strict\",         # ensures exact boundary matching\n",
    "        scheme=\"IOB2\"          # or \"IOB1\", \"IOE2\", etc., if that's your labeling scheme\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",       # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",            # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True,      # Load the best model at the end of training\n",
    "    metric_for_best_model=\"f1\",       # Choose the metric to select the best model\n",
    "    greater_is_better=True,           # True if a higher metric is better\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    # optionally keep only the 1 best checkpoint to save space\n",
    "    save_total_limit=1,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "logger.info(\"Evaluating model on evaluation dataset...\")\n",
    "eval_results = trainer.evaluate()\n",
    "logger.info(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "logger.info(f\"Saving model to {output_dir} ...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "logger.info(\"Training complete.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80db0b9a-f3e6-4780-9d5b-a66e38605336",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def merge_bio_entities(entities):\n",
    "    merged = []\n",
    "    current_label = None\n",
    "    current_tokens = []\n",
    "\n",
    "    for ent in entities:\n",
    "        # Each 'ent' has {\"label\": \"B-DATE\", \"text\": \"...\"} etc.\n",
    "        if \"-\" in ent[\"label\"]:\n",
    "            prefix, ent_type = ent[\"label\"].split(\"-\", 1)\n",
    "        else:\n",
    "            # If there's no dash, treat label as \"O\" or single-label\n",
    "            prefix, ent_type = \"O\", ent[\"label\"]\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            # Close any previously open entity\n",
    "            if current_label is not None:\n",
    "                merged.append({\"label\": current_label, \"text\": \" \".join(current_tokens)})\n",
    "            current_label = ent_type\n",
    "            current_tokens = [ent[\"text\"]]\n",
    "\n",
    "        elif prefix == \"I\" and ent_type == current_label:\n",
    "            # Continue the current entity\n",
    "            current_tokens.append(ent[\"text\"])\n",
    "\n",
    "        else:\n",
    "            # If prefix is \"O\" or label doesn't match the current entity\n",
    "            # close the current entity if open\n",
    "            if current_label is not None:\n",
    "                merged.append({\"label\": current_label, \"text\": \" \".join(current_tokens)})\n",
    "                current_label = None\n",
    "                current_tokens = []\n",
    "            # If this is a new B-XXX label, start a new entity\n",
    "            if prefix == \"B\":\n",
    "                current_label = ent_type\n",
    "                current_tokens = [ent[\"text\"]]\n",
    "\n",
    "    # Close any leftover entity\n",
    "    if current_label is not None:\n",
    "        merged.append({\"label\": current_label, \"text\": \" \".join(current_tokens)})\n",
    "\n",
    "    return merged\n",
    "\n",
    "# -- Main prediction code --\n",
    "\n",
    "test_sentences, test_labels = read_iob(\"test.iob\")\n",
    "test_dataset = NERDataset(test_sentences, test_labels, tokenizer, label_to_id, max_length=max_length)\n",
    "\n",
    "prediction_output = trainer.predict(test_dataset)\n",
    "pred_logits = prediction_output.predictions\n",
    "true_label_ids = prediction_output.label_ids\n",
    "pred_ids = pred_logits.argmax(axis=-1)\n",
    "\n",
    "final_res = []\n",
    "\n",
    "for i, text in enumerate(test_sentences):\n",
    "    # Filter out positions where the label is -100 (subword/padding)\n",
    "    words_index = (true_label_ids[i] != -100)\n",
    "    t_ids = true_label_ids[i][words_index]\n",
    "    p_ids = pred_ids[i][words_index]\n",
    "    \n",
    "    raw_entities = []\n",
    "    for word, t_id, p_id in zip(text, t_ids, p_ids):\n",
    "        predicted_label = id_to_label[p_id]\n",
    "        if predicted_label != \"O\":\n",
    "            # Collect each non-O prediction\n",
    "            raw_entities.append({\"label\": predicted_label, \"text\": word})\n",
    "    \n",
    "    # Merge consecutive B-XXX and I-XXX tokens\n",
    "    merged_entities = merge_bio_entities(raw_entities)\n",
    "    # Convert to a JSON string\n",
    "    final_res.append(json.dumps(merged_entities, ensure_ascii=False))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "396ca624-4a3a-494c-8f7c-d2b26cb72f0e",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(final_res, columns=[\"pred\"]).to_csv(\"pred.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f8cbd903-2dc5-45a5-83ee-096e3df6dbd5",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
