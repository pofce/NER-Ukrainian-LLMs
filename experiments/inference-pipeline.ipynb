{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T10:39:14.167417Z",
     "start_time": "2024-11-13T10:39:14.163843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from utils import load_secrets\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = load_secrets()[\"HF_TOKEN\"]"
   ],
   "id": "464df8ac360b4250",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/spacy_transformers/layers/hf_shim.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/spacy_transformers/layers/hf_shim.py:137: UserWarning: Error loading saved torch state_dict with strict=True, likely due to differences between 'transformers' versions. Attempting to load with strict=False as a fallback...\n",
      "\n",
      "If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current 'transformers' and 'spacy-transformers' versions. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/spacy_transformers/layers/hf_shim.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b = torch.load(filelike, map_location=device)\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"uk_ner_web_trf_13class\")"
   ],
   "id": "5ce432428f830805"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:34:29.233598Z",
     "start_time": "2024-11-02T11:34:10.203134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.spacy\n",
    "from huggingface_hub import login\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "\n",
    "\n",
    "def ner_inference(text):\n",
    "    \"\"\"Perform NER on the input text using spaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = [\n",
    "        {\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'start_char': ent.start_char,\n",
    "            'end_char': ent.end_char\n",
    "        }\n",
    "        for ent in doc.ents\n",
    "    ]\n",
    "    result = {'entities': entities}\n",
    "    return result\n",
    "\n",
    "def ner_inference_mlflow(text, experiment_name):\n",
    "    \"\"\"Run the NER inference and log inputs and outputs using MLflow.\"\"\"\n",
    "    # Set the experiment; create it if it doesn't exist\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"input_text\", text)\n",
    "        try:\n",
    "            result = ner_inference(text)\n",
    "            # Log entities as an artifact instead of a parameter\n",
    "            mlflow.log_dict(result, 'entities.json')\n",
    "            # Log the model only if it hasn't been logged before\n",
    "            if not mlflow.active_run().data.tags.get('model_logged'):\n",
    "                mlflow.spacy.log_model(spacy_model=nlp, artifact_path=\"uk_ner_model\")\n",
    "                mlflow.set_tag('model_logged', 'true')\n",
    "            print(f\"MLflow run ID: {run.info.run_id}\")\n",
    "        except Exception as e:\n",
    "            mlflow.log_param('error', str(e))\n",
    "            print(f\"Error during MLflow logging: {e}\")\n",
    "            raise\n",
    "    return result\n",
    "\n",
    "\n",
    "def save_run_to_hf(input_text, predicted_entities):\n",
    "    \"\"\"Upload the inference data to a Hugging Face Dataset repository.\"\"\"\n",
    "    \n",
    "    token = os.environ.get('HF_TOKEN')\n",
    "    if not token:\n",
    "        raise EnvironmentError(\"HF_TOKEN environment variable not set.\")\n",
    "    login(token=token)\n",
    "    \n",
    "    data = {'input_text': [input_text], 'predicted_entities': [predicted_entities]}\n",
    "    updated_dataset = Dataset.from_dict(data)\n",
    "    \n",
    "    dataset_repo_id = 'pofce/test_inference'\n",
    "    updated_dataset.push_to_hub(dataset_repo_id, token=token)\n",
    "    print(f\"Dataset updated and pushed to the hub: {dataset_repo_id}\")\n",
    "\n",
    "\n",
    "def inference_pipeline(text, experiment_name):\n",
    "    \"\"\"Complete inference pipeline: NER inference, logging, and uploading.\"\"\"\n",
    "    try:\n",
    "        result = ner_inference_mlflow(text, experiment_name)\n",
    "        save_run_to_hf(input_text=text, predicted_entities=result['entities'])\n",
    "        return json.dumps(result, ensure_ascii=False, indent=1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during the inference pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_text = \"–õ—å–≤—ñ–≤ ‚Äî —Å—Ç–æ–ª–∏—Ü—è –£–∫—Ä–∞—ó–Ω–∏. –í–æ–ª–æ–¥–∏–º–∏—Ä –ó–µ–ª–µ–Ω—Å—å–∫–∏–π —î –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–æ–º –£–∫—Ä–∞—ó–Ω–∏.\"\n",
    "    output = inference_pipeline(input_text, \"test_pipeline\")"
   ],
   "id": "d4994a279920dd32",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/UCU_5K2S_Recommender_Systems_Project/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "2024/11/02 13:34:12 WARNING mlflow.spacy: Generating only the spacy flavor for the provided spacy model. This means the model can be loaded back via `mlflow.spacy.load_model`, but cannot be loaded back using pyfunc APIs like `mlflow.pyfunc.load_model` or via the `mlflow models` CLI commands. MLflow will only generate the pyfunc flavor for spacy models containing a pipeline component that is an instance of spacy.pipeline.TextCategorizer.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024/11/02 13:34:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024/11/02 13:34:27 INFO mlflow.tracking._tracking_service.client: üèÉ View run merciful-kit-387 at: http://127.0.0.1:5000/#/experiments/107146422132742469/runs/14be722e813740eb88cd49ffdf69c5de.\n",
      "2024/11/02 13:34:27 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: http://127.0.0.1:5000/#/experiments/107146422132742469.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run ID: 14be722e813740eb88cd49ffdf69c5de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8d268314863496091872f7a81e4a780"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfd21ef923da498f9c5686fd2e40b7a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset updated and pushed to the hub: pofce/test_inference\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "78aea2dcb80013ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
